import {exec, ExecException} from 'child_process';

type TransitionType = 'fade'
    | 'wipeleft'
    | 'wiperight'
    | 'wipeup'
    | 'wipedown'
    | 'slideleft'
    | 'slideright'
    | 'slideup'
    | 'slidedown'
    | 'circlecrop'
    | 'rectcrop'
    | 'distance'
    | 'fadeblack'
    | 'fadewhite'
    | 'radial'
    | 'smoothleft'
    | 'smoothright'
    | 'smoothup'
    | 'smoothdown'
    | 'circleopen'
    | 'circleclose'
    | 'vertopen'
    | 'vertclose'
    | 'horzopen'
    | 'horzclose'
    | 'dissolve'
    | 'pixelize'
    | 'diagtl'
    | 'diagtr'
    | 'diagbl'
    | 'diagbr'
    | 'hlslice'
    | 'hrslice'
    | 'vuslice'
    | 'vdslice'
    | 'hblur'
    | 'fadegrays'
    | 'wipetl'
    | 'wipetr'
    | 'wipebl'
    | 'wipebr'
    | 'squeezeh'
    | 'squeezev'
    | 'zoomin'
    | 'fadefast'
    | 'fadeslow'
    | 'hlwind'
    | 'hrwind'
    | 'vuwind'
    | 'vdwind'
    | 'coverleft'
    | 'coverright'
    | 'coverup'
    | 'coverdown'
    | 'revealleft'
    | 'revealright'
    | 'revealup'
    | 'revealdown';

type TransitionWithDuration = { transition: TransitionType; duration: number };

function getVideoDuration(videoPath:string) {
    return new Promise((resolve, reject) => {
        const command = `ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 "${videoPath}"`;
        console.log(command)

        exec(command, (error, stdout, stderr) => {
            if (error) {
                console.log(`error: ${error.message}`);
                reject(`error: ${error.message}`);
                return;
            }
            if (stderr) {
                console.log(`stderr: ${stderr}`);
                reject(`stderr: ${stderr}`);
                return;
            }
            resolve(parseFloat(stdout.trim()));
        });
    });
}

async function generateFfmpegCommand(videoFiles: string[], output:string, transition:TransitionType | TransitionWithDuration[], transitionDuration:number) {
    try {

        if (videoFiles.length < 2) {
            console.log('Need at least two video files to create transitions.');
            throw new Error('Need at least two video files to create transitions.');
        }

        if (transition instanceof Array && transition.length !== videoFiles.length) {
            console.log('Transition array length should be equal to video files length');
            throw new Error('Transition array length should be equal to video files length');
        }
        let frameRate  = '[0:v]settb=AVTB,fps=30/1[vfps0];';
        let fpsMap = ` -map "[vfps0]" `;

        let filterComplex = '';
        let audioTransitions = '';
        let prevVideo = `[0:v]`; // Corrected to specify the video stream of the first file
        let prevAudio = `[0:a]`; // Corrected to specify the audio stream of the first file

        const fadeDuration = transitionDuration; // Half-second fade
        let lastOffset = 0; // Initialize last offset

        for (let index = 0; index < videoFiles.length - 1; index++) {
            // Only iterate to the second last element
            const duration = (await getVideoDuration(videoFiles[index])) as number;

            const nextIndex = index + 1;
            const videoFade = `[vfade${nextIndex}]`;
            const fpsFade = `[vfps${nextIndex}]`;
            const audioFade = `[afade${nextIndex}]`;

            const xFadeTransition = transition instanceof Array ? transition[index].transition : transition;
            const xFadeDuration = transition instanceof Array ? transition[index].duration : fadeDuration;


            const offset = lastOffset + duration - xFadeDuration;
            lastOffset = offset; // Update last offset used for the next iteration

            frameRate += `[${nextIndex}:v]settb=AVTB,fps=30/1${fpsFade};`;
            filterComplex += `${prevVideo}[${nextIndex}:v]xfade=transition=${xFadeTransition}:duration=${xFadeDuration}:offset=${offset}${videoFade};`;
            audioTransitions += `${prevAudio}[${nextIndex}:a]acrossfade=d=${xFadeDuration}${audioFade};`;
            fpsMap += ` -map "${fpsFade}" `;
            prevVideo = videoFade; // Update previous video to current for the next iteration
            prevAudio = audioFade; // Update previous audio to current for the next iteration
        }

        // Append format setting to the last video fade label used and specify output labels for map
        
        filterComplex += `${prevVideo}concat=n=${videoFiles.length}:v=1:a=1[vout];`;
        audioTransitions += `${prevAudio}aformat=sample_fmts=fltp:sample_rates=44100:channel_layouts=stereo[aout];`;

        const finalCommand = `ffmpeg -y ${videoFiles
            .map((file, index) => `-i "${file}"`)
            .join(
                ' ',
            )} -filter_complex "${frameRate} ${filterComplex}" -map "[vout]" ${fpsMap} "${output}"`;

        return finalCommand;
    } catch (error) {
        console.log('Failed to generate ffmpeg command:', error);
        console.error('Failed to generate ffmpeg command:', error);
        throw error;
    }
}

async function blendVideos(videoPaths: string[], output: string, transition: TransitionType |TransitionWithDuration[], transitionDuration=0.5, callback: { (err: any, result: any): void; (arg0: ExecException, arg1: string): void; }) {
    console.log('Error concatenating videos:')

    try {
        const command = await generateFfmpegCommand(videoPaths, output, transition, transitionDuration);
        console.log(command)

        exec(command, async (error, stdout, stderr) => {

            if (error) {
                console.log('Failed to process videos:', error);
                callback(error, null);
            } else {
                console.log('done');
                callback(null, output);
                return;
            }
        });
    } catch (error) {
        console.log('Failed to process videos:', error);
        if (callback) callback(error, null);
    }
}

export { blendVideos, TransitionType, TransitionWithDuration };
